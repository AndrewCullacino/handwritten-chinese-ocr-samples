{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Chinese OCR - Colab Training (A100 Optimized)\n",
    "\n",
    "Train handwritten Chinese OCR model on Google Colab using CASIA-HWDB 2.0, 2.1, and 2.2 datasets.\n",
    "\n",
    "**Optimized for A100 GPU:**\n",
    "- TF32 precision (8-19x faster than FP32)\n",
    "- Mixed precision training (AMP)\n",
    "- Optimized data loading with prefetching\n",
    "- Large batch sizes for better GPU utilization\n",
    "\n",
    "## Quick Start\n",
    "1. Upload **only data** (`HWDB2.0Train`, `HWDB2.0Test`, `HWDB2.1Train`, `HWDB2.1Test`, `HWDB2.2Train`, `HWDB2.2Test`) to `My Drive/HWDB-data/`\n",
    "2. Code is automatically cloned from GitHub (always up-to-date)\n",
    "3. Open this notebook in Colab with **A100 GPU runtime**\n",
    "4. Run cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU: {gpu_name}')\n",
    "    print(f'Memory: {gpu_memory:.1f} GB')\n",
    "    \n",
    "    if 'A100' in gpu_name:\n",
    "        print('\\n✓ A100 detected! Optimal settings will be used:')\n",
    "        print('  - Batch size: 32-64')\n",
    "        print('  - TF32 precision: enabled')\n",
    "        print('  - Mixed precision (AMP): enabled')\n",
    "    elif 'V100' in gpu_name:\n",
    "        print('\\n✓ V100 detected. Recommended batch size: 16-32')\n",
    "    elif 'T4' in gpu_name:\n",
    "        print('\\n✓ T4 detected. Recommended batch size: 8-16')\n",
    "    else:\n",
    "        print(f'\\n✓ {gpu_name} detected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print('✓ Drive mounted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Project from GitHub\n",
    "\n",
    "Code is cloned fresh from GitHub, so you always get the latest updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old clone if exists\n",
    "!rm -rf handwritten-chinese-ocr-samples\n",
    "\n",
    "# Clone latest code from GitHub\n",
    "!git clone https://github.com/AndrewCullacino/handwritten-chinese-ocr-samples.git\n",
    "%cd handwritten-chinese-ocr-samples\n",
    "\n",
    "print('✓ Latest code cloned from GitHub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print('✓ Dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Link Data from Google Drive\n",
    "\n",
    "**Important:** Your data should be in `My Drive/HWDB-data/` with this structure:\n",
    "```\n",
    "My Drive/HWDB-data/\n",
    "├── HWDB2.0Train/\n",
    "│   ├── *.dgrl files\n",
    "├── HWDB2.0Test/\n",
    "│   ├── *.dgrl files\n",
    "├── HWDB2.1Train/\n",
    "│   ├── *.dgrl files\n",
    "├── HWDB2.1Test/\n",
    "│   ├── *.dgrl files\n",
    "├── HWDB2.2Train/\n",
    "│   ├── *.dgrl files\n",
    "└── HWDB2.2Test/\n",
    "    ├── *.dgrl files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data exists in Drive\n",
    "DATA_DIR = '/content/drive/MyDrive/HWDB-data'\n",
    "\n",
    "# Define all dataset versions to use\n",
    "DATASET_VERSIONS = ['2.0', '2.1', '2.2']\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f'✗ Error: {DATA_DIR} not found')\n",
    "    print('Please upload HWDB2.x datasets to My Drive/HWDB-data/')\n",
    "else:\n",
    "    print(f'Data verification:')\n",
    "    \n",
    "    total_train_dgrl = 0\n",
    "    total_test_dgrl = 0\n",
    "    all_train_exist = True\n",
    "    all_test_exist = True\n",
    "    \n",
    "    for version in DATASET_VERSIONS:\n",
    "        train_dir = f'{DATA_DIR}/HWDB{version}Train'\n",
    "        test_dir = f'{DATA_DIR}/HWDB{version}Test'\n",
    "        \n",
    "        train_exists = os.path.exists(train_dir)\n",
    "        test_exists = os.path.exists(test_dir)\n",
    "        \n",
    "        if not train_exists:\n",
    "            all_train_exist = False\n",
    "        if not test_exists:\n",
    "            all_test_exist = False\n",
    "        \n",
    "        print(f\"\\n  HWDB{version}:\")\n",
    "        print(f\"    {'✓' if train_exists else '✗'} Train: {train_dir}\")\n",
    "        if train_exists:\n",
    "            dgrl_count = len([f for f in os.listdir(train_dir) if f.endswith('.dgrl')])\n",
    "            print(f'      → Found {dgrl_count} .dgrl files')\n",
    "            total_train_dgrl += dgrl_count\n",
    "        \n",
    "        print(f\"    {'✓' if test_exists else '✗'} Test:  {test_dir}\")\n",
    "        if test_exists:\n",
    "            dgrl_count = len([f for f in os.listdir(test_dir) if f.endswith('.dgrl')])\n",
    "            print(f'      → Found {dgrl_count} .dgrl files')\n",
    "            total_test_dgrl += dgrl_count\n",
    "    \n",
    "    print(f'\\n  Summary:')\n",
    "    print(f'    Total train .dgrl files: {total_train_dgrl}')\n",
    "    print(f'    Total test .dgrl files:  {total_test_dgrl}')\n",
    "    \n",
    "    if all_train_exist and all_test_exist:\n",
    "        print('\\n✓ All data ready for preprocessing')\n",
    "    else:\n",
    "        print('\\n⚠ Some datasets are missing. Preprocessing will use available data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocess Dataset\n",
    "\n",
    "Extract text line images from DGRL files using `dgrl2png.py`.\n",
    "\n",
    "**Preprocessed data is saved to Google Drive** (`My Drive/HWDB-data/preprocessed/`) for persistence across sessions. A symlink is created to `data/hwdb2.0/` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed data is saved to Google Drive for persistence\n",
    "# Then symlinked to local path for faster training I/O\n",
    "\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/HWDB-data/preprocessed'\n",
    "LOCAL_DATA_DIR = 'data/hwdb2.x'\n",
    "\n",
    "# Dataset versions to process\n",
    "DATASET_VERSIONS = ['2.0', '2.1', '2.2']\n",
    "\n",
    "# Check if preprocessed data exists in Drive\n",
    "if os.path.exists(f'{DRIVE_DATA_DIR}/train_img_id_gt.txt'):\n",
    "    print('✓ Found preprocessed data in Google Drive')\n",
    "    \n",
    "    # Create symlink to local path for training\n",
    "    !rm -rf {LOCAL_DATA_DIR}\n",
    "    !mkdir -p data\n",
    "    !ln -s {DRIVE_DATA_DIR} {LOCAL_DATA_DIR}\n",
    "    \n",
    "    with open(f'{LOCAL_DATA_DIR}/train_img_id_gt.txt', 'r') as f:\n",
    "        train_samples = len(f.readlines())\n",
    "    with open(f'{LOCAL_DATA_DIR}/val_img_id_gt.txt', 'r') as f:\n",
    "        val_samples = len(f.readlines())\n",
    "    with open(f'{LOCAL_DATA_DIR}/test_img_id_gt.txt', 'r') as f:\n",
    "        test_samples = len(f.readlines())\n",
    "    print(f'  Train: {train_samples} | Val: {val_samples} | Test: {test_samples}')\n",
    "    print(f'  Symlinked: {DRIVE_DATA_DIR} -> {LOCAL_DATA_DIR}')\n",
    "\n",
    "else:\n",
    "    print('Preprocessing DGRL files from HWDB 2.0, 2.1, 2.2 using dgrl2png.py...')\n",
    "    print(f'Output will be saved to: {DRIVE_DATA_DIR}')\n",
    "    \n",
    "    # Create output directories in Google Drive (persistent)\n",
    "    !mkdir -p {DRIVE_DATA_DIR}/train {DRIVE_DATA_DIR}/val {DRIVE_DATA_DIR}/test\n",
    "    \n",
    "    # Process each dataset version\n",
    "    all_train_lines = []\n",
    "    all_test_lines = []\n",
    "    \n",
    "    for idx, version in enumerate(DATASET_VERSIONS):\n",
    "        train_dir = f'/content/drive/MyDrive/HWDB-data/HWDB{version}Train'\n",
    "        test_dir = f'/content/drive/MyDrive/HWDB-data/HWDB{version}Test'\n",
    "        \n",
    "        # Extract training data\n",
    "        if os.path.exists(train_dir):\n",
    "            print(f'\\n[{idx*2 + 1}/{len(DATASET_VERSIONS)*2}] Extracting HWDB{version} training data...')\n",
    "            !python utils/casia-hwdb-data-preparation/dgrl2png.py \\\n",
    "                {train_dir} \\\n",
    "                {DRIVE_DATA_DIR}/train \\\n",
    "                --image_height 128\n",
    "            \n",
    "            # Read and collect ground truth\n",
    "            gt_file = f'{DRIVE_DATA_DIR}/train/dgrl_img_gt.txt'\n",
    "            if os.path.exists(gt_file):\n",
    "                with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "                    lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "                    all_train_lines.extend(lines)\n",
    "                print(f'    → Collected {len(lines)} samples')\n",
    "        else:\n",
    "            print(f'\\n⚠ Skipping HWDB{version}Train (not found)')\n",
    "        \n",
    "        # Extract test data\n",
    "        if os.path.exists(test_dir):\n",
    "            print(f'\\n[{idx*2 + 2}/{len(DATASET_VERSIONS)*2}] Extracting HWDB{version} test data...')\n",
    "            !python utils/casia-hwdb-data-preparation/dgrl2png.py \\\n",
    "                {test_dir} \\\n",
    "                {DRIVE_DATA_DIR}/test \\\n",
    "                --image_height 128\n",
    "            \n",
    "            # Read and collect ground truth\n",
    "            gt_file = f'{DRIVE_DATA_DIR}/test/dgrl_img_gt.txt'\n",
    "            if os.path.exists(gt_file):\n",
    "                with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "                    lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "                    all_test_lines.extend(lines)\n",
    "                print(f'    → Collected {len(lines)} samples')\n",
    "        else:\n",
    "            print(f'\\n⚠ Skipping HWDB{version}Test (not found)')\n",
    "    \n",
    "    # Create train/val split from all collected training data\n",
    "    print('\\n[Final] Creating train/val split from all datasets...')\n",
    "    \n",
    "    import random\n",
    "    import shutil\n",
    "    \n",
    "    # Shuffle and split (90% train, 10% val)\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_train_lines)\n",
    "    \n",
    "    val_size = int(len(all_train_lines) * 0.1)\n",
    "    val_lines = all_train_lines[:val_size]\n",
    "    train_lines = all_train_lines[val_size:]\n",
    "    \n",
    "    # Move val images to val folder\n",
    "    print(f'  Moving {len(val_lines)} images to validation set...')\n",
    "    for line in val_lines:\n",
    "        img_name = line.split(',')[0]\n",
    "        src = f'{DRIVE_DATA_DIR}/train/{img_name}'\n",
    "        dst = f'{DRIVE_DATA_DIR}/val/{img_name}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    # Write metadata files to Drive\n",
    "    with open(f'{DRIVE_DATA_DIR}/train_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_lines))\n",
    "    \n",
    "    with open(f'{DRIVE_DATA_DIR}/val_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(val_lines))\n",
    "    \n",
    "    with open(f'{DRIVE_DATA_DIR}/test_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_test_lines))\n",
    "    \n",
    "    # Generate character list from all data\n",
    "    all_chars = set()\n",
    "    for line in train_lines + val_lines + all_test_lines:\n",
    "        if ',' in line:\n",
    "            text = line.split(',', 1)[1]\n",
    "            all_chars.update(text)\n",
    "    \n",
    "    with open(f'{DRIVE_DATA_DIR}/chars_list.txt', 'w', encoding='utf-8') as f:\n",
    "        for char in sorted(all_chars):\n",
    "            f.write(char + '\\n')\n",
    "    \n",
    "    # Create symlink for training\n",
    "    !rm -rf {LOCAL_DATA_DIR}\n",
    "    !mkdir -p data\n",
    "    !ln -s {DRIVE_DATA_DIR} {LOCAL_DATA_DIR}\n",
    "    \n",
    "    print(f'\\n✓ Preprocessing complete - saved to Google Drive')\n",
    "    print(f'  Train: {len(train_lines)} | Val: {len(val_lines)} | Test: {len(all_test_lines)}')\n",
    "    print(f'  Characters: {len(all_chars)}')\n",
    "    print(f'  Location: {DRIVE_DATA_DIR}')\n",
    "    print(f'  Symlinked to: {LOCAL_DATA_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/hwdb2.x'\n",
    "\n",
    "if os.path.exists(f'{DATASET_PATH}/train_img_id_gt.txt'):\n",
    "    # Count samples per split\n",
    "    splits = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        gt_file = f'{DATASET_PATH}/{split}_img_id_gt.txt'\n",
    "        if os.path.exists(gt_file):\n",
    "            with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "                splits[split] = len(f.readlines())\n",
    "    \n",
    "    # Count characters\n",
    "    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n",
    "        chars = len(f.readlines())\n",
    "    \n",
    "    print(f'Dataset ready (HWDB 2.0 + 2.1 + 2.2):')\n",
    "    print(f'  Train: {splits.get(\"train\", 0):,} samples')\n",
    "    print(f'  Val:   {splits.get(\"val\", 0):,} samples')\n",
    "    print(f'  Test:  {splits.get(\"test\", 0):,} samples')\n",
    "    print(f'  Character vocab: {chars:,}')\n",
    "    print(f'  Location: {DATASET_PATH}/')\n",
    "else:\n",
    "    print('✗ Dataset not found. Please run preprocessing first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "Train the model with A100-optimized settings:\n",
    "- **TF32 precision**: 8-19x faster than FP32 on A100\n",
    "- **Mixed precision (AMP)**: Further speedup with FP16/BF16\n",
    "- **Large batch sizes**: Better GPU utilization\n",
    "- **Gradient clipping**: Prevents exploding gradients in RNN\n",
    "\n",
    "Checkpoints will be saved locally in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DATASET_PATH = 'data/hwdb2.x'\n",
    "\n",
    "# Auto-detect optimal batch size based on GPU\n",
    "# Note: max_width=1200 is enforced in main.py to prevent OOM on long text lines\n",
    "# The model is large (~38M params), so conservative batch sizes are needed\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else ''\n",
    "if 'A100' in gpu_name:\n",
    "    BATCH_SIZE = 16        # A100 40GB: 16-24 (conservative for large model)\n",
    "    NUM_WORKERS = 4\n",
    "    print(f'✓ A100 detected - using batch_size={BATCH_SIZE}')\n",
    "elif 'V100' in gpu_name:\n",
    "    BATCH_SIZE = 12        # V100 16GB: 8-12\n",
    "    NUM_WORKERS = 4\n",
    "    print(f'✓ V100 detected - using batch_size={BATCH_SIZE}')\n",
    "elif 'T4' in gpu_name:\n",
    "    BATCH_SIZE = 8         # T4 16GB: 4-8\n",
    "    NUM_WORKERS = 2\n",
    "    print(f'✓ T4 detected - using batch_size={BATCH_SIZE}')\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 2\n",
    "    print(f'Using default batch_size={BATCH_SIZE}')\n",
    "\n",
    "EPOCHS = 50             # Recommended: 30-50 epochs\n",
    "PRINT_FREQ = 100        # Print every N batches\n",
    "VAL_FREQ = 5000         # Validate every N batches\n",
    "\n",
    "print(f'\\nTraining configuration:')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Epochs: {EPOCHS}')\n",
    "print(f'  Workers: {NUM_WORKERS}')\n",
    "print(f'  Dataset: {DATASET_PATH} (HWDB 2.0 + 2.1 + 2.2)')\n",
    "print(f'  Max image width: 1200 (enforced in main.py)')\n",
    "\n",
    "!python main.py -m hctr \\\n",
    "    -d {DATASET_PATH} \\\n",
    "    -b {BATCH_SIZE} \\\n",
    "    -ep {EPOCHS} \\\n",
    "    -pf {PRINT_FREQ} \\\n",
    "    -vf {VAL_FREQ} \\\n",
    "    -j {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "model_files = glob.glob('hctr_*.pth.tar')\n",
    "\n",
    "if model_files:\n",
    "    print('Saved models:')\n",
    "    for f in sorted(model_files):\n",
    "        size_mb = os.path.getsize(f) / (1024*1024)\n",
    "        print(f'  {f} ({size_mb:.1f} MB)')\n",
    "    \n",
    "    acc_models = [f for f in model_files if 'acc' in f]\n",
    "    BEST_MODEL = sorted(acc_models)[-1] if acc_models else 'hctr_checkpoint.pth.tar'\n",
    "    print(f'\\n✓ Best model: {BEST_MODEL}')\n",
    "else:\n",
    "    print('No models found')\n",
    "    BEST_MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/hwdb2.x'\n",
    "\n",
    "if BEST_MODEL and os.path.exists(BEST_MODEL):\n",
    "    print(f'Evaluating: {BEST_MODEL}\\n')\n",
    "    !python test.py -m hctr \\\n",
    "        -f {BEST_MODEL} \\\n",
    "        -i {DATASET_PATH} \\\n",
    "        -b 16 \\\n",
    "        -bm \\\n",
    "        -dm greedy-search \\\n",
    "        -pf 20\n",
    "else:\n",
    "    print('Model not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Checkpoints to Drive\n",
    "\n",
    "**Important:** Save models to Drive to prevent loss when Colab disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save to Drive\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/HWDB-data/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "model_files = glob.glob('hctr_*.pth.tar')\n",
    "if model_files:\n",
    "    for f in model_files:\n",
    "        dst = os.path.join(CHECKPOINT_DIR, f)\n",
    "        shutil.copy2(f, dst)\n",
    "        print(f'✓ Saved: {dst}')\n",
    "    print(f'\\n✓ All models saved to Drive: {CHECKPOINT_DIR}')\n",
    "else:\n",
    "    print('No models to save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume Training (Optional)\n",
    "\n",
    "If Colab disconnects, run cells 1-7 to restore environment, then run this cell to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoint from Drive\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/HWDB-data/checkpoints'\n",
    "checkpoint_file = 'hctr_checkpoint.pth.tar'\n",
    "\n",
    "if os.path.exists(f'{CHECKPOINT_DIR}/{checkpoint_file}'):\n",
    "    !cp {CHECKPOINT_DIR}/{checkpoint_file} .\n",
    "    print(f'✓ Restored checkpoint: {checkpoint_file}')\n",
    "    \n",
    "    # Detect GPU and set batch size (conservative values for large model)\n",
    "    import torch\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else ''\n",
    "    BATCH_SIZE = 16 if 'A100' in gpu_name else (12 if 'V100' in gpu_name else 8)\n",
    "    \n",
    "    # Resume training with combined dataset\n",
    "    !python main.py -m hctr \\\n",
    "        -d data/hwdb2.x \\\n",
    "        -b {BATCH_SIZE} \\\n",
    "        -ep 100 \\\n",
    "        -pf 100 \\\n",
    "        -vf 5000 \\\n",
    "        -j 4 \\\n",
    "        -re {checkpoint_file}\n",
    "else:\n",
    "    print(f'✗ Checkpoint not found in {CHECKPOINT_DIR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
