{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0658b13d",
   "metadata": {},
   "source": [
    "# Handwritten Chinese OCR - Colab Training\n",
    "\n",
    "æœ¬ Notebook ç”¨äºåœ¨ Google Colab ä¸Šè®­ç»ƒæ‰‹å†™ä¸­æ–‡ OCR æ¨¡å‹ (CASIA-HWDB2.0 æ•°æ®é›†)ã€‚\n",
    "\n",
    "## ä½¿ç”¨æ­¥éª¤\n",
    "1. å°†æ•´ä¸ªé¡¹ç›®æ–‡ä»¶å¤¹ï¼ˆåŒ…å« `HWDB2.0Train` å’Œ `HWDB2.0Test`ï¼‰ä¸Šä¼ åˆ° Google Drive\n",
    "2. åœ¨ Colab ä¸­æ‰“å¼€æ­¤ Notebookï¼Œé€‰æ‹© GPU è¿è¡Œæ—¶\n",
    "3. æŒ‰é¡ºåºè¿è¡Œæ¯ä¸ª Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb344ab",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"âœ… Google Drive already mounted\")\n",
    "else:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f70268",
   "metadata": {},
   "source": [
    "## 2. Set Project Path\n",
    "\n",
    "**âš ï¸ Modify `PROJECT_PATH` below to match your Google Drive folder location.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ MODIFY THIS PATH to your project folder in Google Drive\n",
    "PROJECT_PATH = '/content/drive/MyDrive/handwritten-chinese-ocr-samples'\n",
    "\n",
    "# Change to project directory\n",
    "%cd {PROJECT_PATH}\n",
    "\n",
    "# Verify project structure\n",
    "!ls -la\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"âœ… Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b5918",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1caa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555b6c3",
   "metadata": {},
   "source": [
    "## 3.5 Update Code Files (Important Fix!)\n",
    "\n",
    "**âš ï¸ The original model had a CTC bug causing Loss=0.0000 and no learning.**\n",
    "\n",
    "The fix: Reduced maxpool layers from 5â†’4 so the output sequence length is long enough for CTC loss to work with Chinese text labels (40+ characters).\n",
    "\n",
    "Run the cell below to download the fixed files from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download fixed files from GitHub\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/AndrewCullacino/handwritten-chinese-ocr-samples/main/\"\n",
    "\n",
    "files_to_update = [\n",
    "    \"models/handwritten_ctr_model.py\",  # Fixed: 4 maxpool instead of 5\n",
    "    \"main.py\",                           # Added CTC debug logging\n",
    "]\n",
    "\n",
    "print(\"Updating code files from GitHub...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for f in files_to_update:\n",
    "    url = base_url + f\n",
    "    dest = f\n",
    "    os.makedirs(os.path.dirname(dest) if os.path.dirname(dest) else '.', exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(f\"âœ… Updated: {f}\")\n",
    "\n",
    "# Delete old checkpoints (incompatible with new model architecture)\n",
    "import glob\n",
    "old_models = glob.glob('hctr_*.pth.tar')\n",
    "if old_models:\n",
    "    print(f\"\\nâš ï¸ Found {len(old_models)} old model checkpoints.\")\n",
    "    print(\"   These are incompatible with the fixed model (different layer sizes).\")\n",
    "    confirm = input(\"   Delete old checkpoints? (y/n): \").strip().lower()\n",
    "    if confirm == 'y':\n",
    "        for f in old_models:\n",
    "            os.remove(f)\n",
    "            print(f\"   ğŸ—‘ï¸ Deleted: {f}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ Keep old models. Training will fail if you try to resume from them.\")\n",
    "\n",
    "print(\"\\nâœ… Code update complete! Now run training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae50ec",
   "metadata": {},
   "source": [
    "## 4. Data Preparation Notes\n",
    "\n",
    "### âš ï¸ IMPORTANT: Dataset Preprocessing Required\n",
    "\n",
    "Before training, the raw **CASIA-HWDB2.x** binary files (`.dgrl`, `.dgr`, `.gnt` formats) must be preprocessed into image files (PNG) and label files.\n",
    "\n",
    "### Preprocessing Utilities\n",
    "\n",
    "The following utility scripts are available in `utils/casia-hwdb-data-preparation/`:\n",
    "\n",
    "| Script | Purpose | Input Format |\n",
    "|--------|---------|-------------|\n",
    "| `preprocess_dgrl.py` | Convert DGRL text line files to PNG + labels | `.dgrl` |\n",
    "| `dgr2png.c` | C/C++ utility for converting DGR files | `.dgr` |\n",
    "| `gnt2png.py` | Convert GNT character files to PNG | `.gnt` |\n",
    "\n",
    "### Expected Dataset Structure After Preprocessing\n",
    "\n",
    "```\n",
    "data/hwdb2.0/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ 000000.png\n",
    "â”‚   â”œâ”€â”€ 000001.png\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ val/\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ test/\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ train_img_id_gt.txt    # Format: image_name,label_text\n",
    "â”œâ”€â”€ val_img_id_gt.txt\n",
    "â”œâ”€â”€ test_img_id_gt.txt\n",
    "â””â”€â”€ chars_list.txt         # One character per line\n",
    "```\n",
    "\n",
    "### Example: Preprocessing DGRL Files\n",
    "\n",
    "```bash\n",
    "# Run from project root\n",
    "python preprocess_dgrl.py \\\n",
    "    --train_dir HWDB2.0Train \\\n",
    "    --test_dir HWDB2.0Test \\\n",
    "    --output_dir data/hwdb2.0 \\\n",
    "    --val_split 0.1\n",
    "```\n",
    "\n",
    "### Compiling dgr2png.c (if needed)\n",
    "\n",
    "```bash\n",
    "cd utils/casia-hwdb-data-preparation\n",
    "gcc -o dgr2png dgr2png.c\n",
    "./dgr2png <input.dgr> <output_dir>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a7ae0",
   "metadata": {},
   "source": [
    "## 5. Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path (modify if using a different location)\n",
    "DATASET_PATH = 'data/hwdb2.0'  # or 'data/handwritten_ctr_data'\n",
    "\n",
    "# Check dataset files\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    f'{DATASET_PATH}/train_img_id_gt.txt',\n",
    "    f'{DATASET_PATH}/val_img_id_gt.txt', \n",
    "    f'{DATASET_PATH}/test_img_id_gt.txt',\n",
    "    f'{DATASET_PATH}/chars_list.txt',\n",
    "    f'{DATASET_PATH}/train',\n",
    "    f'{DATASET_PATH}/val',\n",
    "    f'{DATASET_PATH}/test'\n",
    "]\n",
    "\n",
    "print(\"Dataset verification:\")\n",
    "print(\"=\"*50)\n",
    "all_ok = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = \"âœ…\" if exists else \"âŒ\"\n",
    "    print(f\"{status} {f}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    # Count samples\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        gt_file = f'{DATASET_PATH}/{split}_img_id_gt.txt'\n",
    "        with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "            count = len(f.readlines())\n",
    "        print(f\"   {split}: {count} samples\")\n",
    "    \n",
    "    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n",
    "        num_chars = len(f.readlines())\n",
    "    print(f\"   Character vocabulary: {num_chars}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some files are missing. Please preprocess the dataset first.\")\n",
    "    print(\"   See 'Data Preparation Notes' section above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc74d43",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "DATASET_PATH = 'data/hwdb2.0'  # Path to preprocessed dataset\n",
    "BATCH_SIZE = 8                  # Adjust based on GPU memory\n",
    "EPOCHS = 30                     # Recommended: 30+ epochs for good CER\n",
    "PRINT_FREQ = 50                 # Print frequency\n",
    "NUM_WORKERS = 2                 # Data loading workers\n",
    "\n",
    "# Run training\n",
    "# You should see:\n",
    "#   - [CTC Debug] showing output seq_len > max target len (GOOD!)\n",
    "#   - Loss > 0 and decreasing (model is learning!)\n",
    "#   - CER decreasing over epochs\n",
    "!python main.py -m hctr \\\n",
    "    -d {DATASET_PATH} \\\n",
    "    -b {BATCH_SIZE} \\\n",
    "    -ep {EPOCHS} \\\n",
    "    -pf {PRINT_FREQ} \\\n",
    "    -j {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922ecf1",
   "metadata": {},
   "source": [
    "## 7. Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find saved model files\n",
    "model_files = glob.glob('hctr_*.pth.tar')\n",
    "\n",
    "if model_files:\n",
    "    print(\"Saved models:\")\n",
    "    for f in sorted(model_files):\n",
    "        size_mb = os.path.getsize(f) / (1024*1024)\n",
    "        print(f\"  ğŸ“ {f} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Select best model (with 'acc' in name, or latest checkpoint)\n",
    "    acc_models = [f for f in model_files if 'acc' in f]\n",
    "    if acc_models:\n",
    "        BEST_MODEL = sorted(acc_models)[-1]\n",
    "    else:\n",
    "        BEST_MODEL = 'hctr_checkpoint.pth.tar'\n",
    "    \n",
    "    print(f\"\\nâœ… Selected model: {BEST_MODEL}\")\n",
    "else:\n",
    "    print(\"âŒ No model files found. Please run training first.\")\n",
    "    BEST_MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb7c3f",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3447d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "DATASET_PATH = 'data/hwdb2.0'\n",
    "MODEL_FILE = BEST_MODEL  # Or specify manually: 'hctr_checkpoint.pth.tar'\n",
    "TEST_PATH = f'{DATASET_PATH}/test'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "if MODEL_FILE and os.path.exists(MODEL_FILE):\n",
    "    print(f\"Evaluating model: {MODEL_FILE}\")\n",
    "    print(f\"Test set: {TEST_PATH}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Run evaluation with benchmark mode (-bm)\n",
    "    !python test.py -m hctr \\\n",
    "        -f {MODEL_FILE} \\\n",
    "        -i {TEST_PATH} \\\n",
    "        -b {BATCH_SIZE} \\\n",
    "        -bm \\\n",
    "        -dm greedy-search \\\n",
    "        -pf 20\n",
    "else:\n",
    "    print(\"âŒ Model file not found. Please run training first or specify the correct path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed954621",
   "metadata": {},
   "source": [
    "## 9. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c20f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from models.handwritten_ctr_model import hctr_model\n",
    "from utils.ctc_codec import ctc_codec\n",
    "from utils.dataset import NormalizePAD\n",
    "\n",
    "DATASET_PATH = 'data/hwdb2.0'\n",
    "\n",
    "if MODEL_FILE and os.path.exists(MODEL_FILE):\n",
    "    # Load model\n",
    "    model = hctr_model()\n",
    "    checkpoint = torch.load(MODEL_FILE, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load character set\n",
    "    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n",
    "        characters = ''.join([line.strip() for line in f])\n",
    "    codec = ctc_codec(characters)\n",
    "    \n",
    "    # Load test labels\n",
    "    test_labels = {}\n",
    "    with open(f'{DATASET_PATH}/test_img_id_gt.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',', 1)\n",
    "            if len(parts) == 2:\n",
    "                test_labels[parts[0]] = parts[1]\n",
    "    \n",
    "    # Random samples\n",
    "    samples = random.sample(list(test_labels.keys()), min(4, len(test_labels)))\n",
    "    \n",
    "    fig, axes = plt.subplots(len(samples), 1, figsize=(15, 3*len(samples)))\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, img_name in zip(axes, samples):\n",
    "        img_path = f'{DATASET_PATH}/test/{img_name}'\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Predict\n",
    "        h, w = img.shape\n",
    "        img_tensor = img[:, :, None]\n",
    "        transform = NormalizePAD((1, 128, w))\n",
    "        img_tensor = transform(img_tensor).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(img_tensor)\n",
    "            result = codec.decode(preds.cpu().numpy())[0]\n",
    "        \n",
    "        true_label = test_labels[img_name]\n",
    "        match = \"âœ…\" if result == true_label else \"âŒ\"\n",
    "        \n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f\"{match} GT: {true_label}\\nPred: {result}\", fontsize=10, loc='left')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ Model not available for prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d4d75",
   "metadata": {},
   "source": [
    "## 10. Save Model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f661f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create checkpoints directory in Drive\n",
    "SAVE_DIR = f'{PROJECT_PATH}/checkpoints'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Copy all model files\n",
    "model_files = glob.glob('hctr_*.pth.tar')\n",
    "for f in model_files:\n",
    "    dst = os.path.join(SAVE_DIR, f)\n",
    "    shutil.copy2(f, dst)\n",
    "    print(f\"âœ… Saved: {dst}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ All models saved to: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55913cfd",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca593f6",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb10de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92a7b2",
   "metadata": {},
   "source": [
    "## 3. è®¾ç½®è·¯å¾„å¹¶å¤åˆ¶é¡¹ç›®\n",
    "\n",
    "âš ï¸ **ä¿®æ”¹ä¸‹é¢çš„è·¯å¾„ä¸ºä½  Google Drive ä¸­çš„å®é™…ä½ç½®**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ä¿®æ”¹è¿™é‡Œ ==========\n",
    "DRIVE_PROJECT = '/content/drive/MyDrive/handwritten-chinese-ocr-samples'\n",
    "# ==============================\n",
    "\n",
    "# æŸ¥çœ‹ Drive æ ¹ç›®å½•å¸®åŠ©å®šä½\n",
    "!ls /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤åˆ¶é¡¹ç›®åˆ° Colab æœ¬åœ° (æ›´å¿«çš„è¯»å†™é€Ÿåº¦)\n",
    "!cp -r \"{DRIVE_PROJECT}\" /content/\n",
    "%cd /content/handwritten-chinese-ocr-samples\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b152a",
   "metadata": {},
   "source": [
    "## 4. å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef42491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q editdistance\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0da51",
   "metadata": {},
   "source": [
    "## 5. åˆ›å»º DGRL é¢„å¤„ç†è„šæœ¬\n",
    "\n",
    "è¿™æ˜¯ä¿®å¤åçš„è„šæœ¬ï¼Œå¯ä»¥æ­£ç¡®è§£æ CASIA-HWDB2.0 çš„ DGRL æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ff73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocess_dgrl.py\n",
    "\"\"\"\n",
    "DGRL Preprocessor for CASIA-HWDB2.x\n",
    "Converts .dgrl files to PNG images + labels for OCR training.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse_dgrl_file(filepath, debug=False):\n",
    "    \"\"\"Parse a single DGRL file and extract text lines with images.\"\"\"\n",
    "    lines_data = []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        if len(data) < 85:\n",
    "            return lines_data\n",
    "        \n",
    "        # Header: 73 bytes, then page info\n",
    "        pos = 73\n",
    "        page_w = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "        pos += 4\n",
    "        page_h = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "        pos += 4\n",
    "        num_lines = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "        pos += 4\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  Page: {page_w}x{page_h}, {num_lines} lines\")\n",
    "        \n",
    "        if num_lines > 50 or num_lines == 0:\n",
    "            return lines_data\n",
    "        \n",
    "        for line_idx in range(num_lines):\n",
    "            if pos + 4 > len(data):\n",
    "                break\n",
    "            \n",
    "            num_chars = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "            pos += 4\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"  Line {line_idx}: num_chars={num_chars}\")\n",
    "            \n",
    "            if num_chars > 200 or num_chars == 0:\n",
    "                break\n",
    "            \n",
    "            # Read GB codes (2 bytes each)\n",
    "            text = ''\n",
    "            for _ in range(num_chars):\n",
    "                if pos + 2 > len(data):\n",
    "                    break\n",
    "                code = data[pos:pos+2]\n",
    "                pos += 2\n",
    "                try:\n",
    "                    char = code.decode('gb18030')\n",
    "                    text += char\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if pos + 8 > len(data):\n",
    "                break\n",
    "            \n",
    "            # Image dimensions\n",
    "            height = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "            pos += 4\n",
    "            width = struct.unpack('<I', data[pos:pos+4])[0]\n",
    "            pos += 4\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"    Image: {width}x{height}, text: {text[:20]}...\")\n",
    "            \n",
    "            if height > 2000 or width > 8000 or height == 0 or width == 0:\n",
    "                break\n",
    "            \n",
    "            img_size = height * width\n",
    "            if pos + img_size > len(data):\n",
    "                break\n",
    "            \n",
    "            img_array = np.frombuffer(data[pos:pos+img_size], dtype=np.uint8)\n",
    "            img_array = img_array.reshape(height, width)\n",
    "            pos += img_size\n",
    "            \n",
    "            # KEY: Skip trailing line data until 0xFF\n",
    "            while pos < len(data) and data[pos] != 0xFF:\n",
    "                pos += 1\n",
    "            \n",
    "            # Skip 0xFF padding\n",
    "            while pos < len(data) and data[pos] == 0xFF:\n",
    "                pos += 1\n",
    "            \n",
    "            if text:\n",
    "                lines_data.append({\n",
    "                    'image': img_array,\n",
    "                    'text': text,\n",
    "                    'height': height,\n",
    "                    'width': width\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    return lines_data\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_dir, phase, target_height=128):\n",
    "    \"\"\"Process all DGRL files in a directory.\"\"\"\n",
    "    os.makedirs(os.path.join(output_dir, phase), exist_ok=True)\n",
    "    \n",
    "    dgrl_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.dgrl')])\n",
    "    print(f\"Found {len(dgrl_files)} DGRL files in {input_dir}\")\n",
    "    \n",
    "    all_pairs = []\n",
    "    all_chars = set()\n",
    "    \n",
    "    for i, dgrl_file in enumerate(dgrl_files):\n",
    "        filepath = os.path.join(input_dir, dgrl_file)\n",
    "        lines = parse_dgrl_file(filepath)\n",
    "        \n",
    "        for line_idx, line_data in enumerate(lines):\n",
    "            img = line_data['image']\n",
    "            h, w = img.shape\n",
    "            ratio = target_height / h\n",
    "            new_w = max(1, int(w * ratio))\n",
    "            \n",
    "            pil_img = Image.fromarray(img)\n",
    "            pil_img = pil_img.resize((new_w, target_height), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            img_name = f\"{dgrl_file.replace('.dgrl', '')}_{line_idx:03d}.png\"\n",
    "            pil_img.save(os.path.join(output_dir, phase, img_name))\n",
    "            \n",
    "            all_pairs.append((img_name, line_data['text']))\n",
    "            all_chars.update(line_data['text'])\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(dgrl_files)}, {len(all_pairs)} lines\")\n",
    "    \n",
    "    gt_file = os.path.join(output_dir, f'{phase}_img_id_gt.txt')\n",
    "    with open(gt_file, 'w', encoding='utf-8') as f:\n",
    "        for img_name, text in all_pairs:\n",
    "            f.write(f'{img_name},{text}\\n')\n",
    "    \n",
    "    print(f\"{phase}: {len(all_pairs)} lines extracted\")\n",
    "    return all_chars\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_dir', default='HWDB2.0Train')\n",
    "    parser.add_argument('--test_dir', default='HWDB2.0Test')\n",
    "    parser.add_argument('--output_dir', default='data/hwdb2.0')\n",
    "    parser.add_argument('--val_split', type=float, default=0.1)\n",
    "    parser.add_argument('--target_height', type=int, default=128)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    all_chars = set()\n",
    "    \n",
    "    # Process training data\n",
    "    if os.path.isdir(args.train_dir):\n",
    "        print(\"Processing training data...\")\n",
    "        chars = process_directory(args.train_dir, args.output_dir, 'train', args.target_height)\n",
    "        all_chars.update(chars)\n",
    "        \n",
    "        # Split validation\n",
    "        train_gt = os.path.join(args.output_dir, 'train_img_id_gt.txt')\n",
    "        if os.path.exists(train_gt):\n",
    "            with open(train_gt, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            import random\n",
    "            random.seed(42)\n",
    "            random.shuffle(lines)\n",
    "            \n",
    "            val_size = int(len(lines) * args.val_split)\n",
    "            val_lines, train_lines = lines[:val_size], lines[val_size:]\n",
    "            \n",
    "            val_dir = os.path.join(args.output_dir, 'val')\n",
    "            train_dir = os.path.join(args.output_dir, 'train')\n",
    "            os.makedirs(val_dir, exist_ok=True)\n",
    "            \n",
    "            for line in val_lines:\n",
    "                img_name = line.strip().split(',')[0]\n",
    "                src = os.path.join(train_dir, img_name)\n",
    "                dst = os.path.join(val_dir, img_name)\n",
    "                if os.path.exists(src):\n",
    "                    os.rename(src, dst)\n",
    "            \n",
    "            with open(train_gt, 'w', encoding='utf-8') as f:\n",
    "                f.writelines(train_lines)\n",
    "            with open(os.path.join(args.output_dir, 'val_img_id_gt.txt'), 'w', encoding='utf-8') as f:\n",
    "                f.writelines(val_lines)\n",
    "            \n",
    "            print(f\"Split: {len(train_lines)} train, {len(val_lines)} val\")\n",
    "    \n",
    "    # Process test data\n",
    "    if os.path.isdir(args.test_dir):\n",
    "        print(\"Processing test data...\")\n",
    "        chars = process_directory(args.test_dir, args.output_dir, 'test', args.target_height)\n",
    "        all_chars.update(chars)\n",
    "    \n",
    "    # Save char list\n",
    "    with open(os.path.join(args.output_dir, 'chars_list.txt'), 'w', encoding='utf-8') as f:\n",
    "        for char in sorted(all_chars):\n",
    "            f.write(char + '\\n')\n",
    "    \n",
    "    print(f\"\\nDone! {len(all_chars)} unique characters\")\n",
    "    print(f\"Output: {args.output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923747e",
   "metadata": {},
   "source": [
    "## 6. æµ‹è¯•é¢„å¤„ç†è„šæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2009294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è§£æå•ä¸ªæ–‡ä»¶\n",
    "import os\n",
    "from preprocess_dgrl import parse_dgrl_file\n",
    "\n",
    "dgrl_files = sorted([f for f in os.listdir('HWDB2.0Train') if f.endswith('.dgrl')])\n",
    "print(f\"Found {len(dgrl_files)} dgrl files\")\n",
    "\n",
    "test_file = f'HWDB2.0Train/{dgrl_files[0]}'\n",
    "print(f\"\\nTesting: {test_file}\")\n",
    "lines = parse_dgrl_file(test_file, debug=True)\n",
    "print(f\"\\nâœ… Extracted {len(lines)} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b77880",
   "metadata": {},
   "source": [
    "## 7. è¿è¡Œå®Œæ•´é¢„å¤„ç† â°\n",
    "\n",
    "è¿™ä¸€æ­¥éœ€è¦ 10-20 åˆ†é’Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86876e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ é™¤æ—§æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "!rm -rf data/hwdb2.0\n",
    "\n",
    "# è¿è¡Œé¢„å¤„ç†\n",
    "!python preprocess_dgrl.py --train_dir HWDB2.0Train --test_dir HWDB2.0Test --output_dir data/hwdb2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9fff1",
   "metadata": {},
   "source": [
    "## 8. æ£€æŸ¥æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== æ•°æ®é›†ç»Ÿè®¡ ===\")\n",
    "!wc -l data/hwdb2.0/*_img_id_gt.txt\n",
    "\n",
    "print(\"\\n=== æ ·æœ¬ç¤ºä¾‹ ===\")\n",
    "!head -3 data/hwdb2.0/train_img_id_gt.txt\n",
    "\n",
    "print(\"\\n=== å­—ç¬¦è¡¨å¤§å° ===\")\n",
    "!wc -l data/hwdb2.0/chars_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ ·æœ¬\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "with open('data/hwdb2.0/train_img_id_gt.txt', 'r', encoding='utf-8') as f:\n",
    "    samples = [line.strip().split(',', 1) for line in f.readlines()[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 5))\n",
    "for i, (img_name, text) in enumerate(samples):\n",
    "    img_path = os.path.join('data/hwdb2.0/train', img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(text[:40] + '...' if len(text) > 40 else text)\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92253b49",
   "metadata": {},
   "source": [
    "## 9. å¼€å§‹è®­ç»ƒ ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "# -b 16: batch size (å¯æ ¹æ® GPU å†…å­˜è°ƒæ•´)\n",
    "# -ep 30: è®­ç»ƒ 30 è½®\n",
    "# -pf 100: æ¯ 100 æ­¥æ‰“å°ä¸€æ¬¡\n",
    "!python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 30 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb3a01",
   "metadata": {},
   "source": [
    "## 10. ä¿å­˜åˆ° Google Drive âš ï¸\n",
    "\n",
    "Colab ä¼šè¯å¯èƒ½éšæ—¶æ–­å¼€ï¼ŒåŠæ—¶ä¿å­˜æ¨¡å‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17945a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ checkpoint åˆ° Drive\n",
    "!mkdir -p /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints\n",
    "!cp hctr_checkpoint.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/\n",
    "!cp hctr_*acc*.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/ 2>/dev/null || true\n",
    "\n",
    "# ä¿å­˜é¢„å¤„ç†æ•°æ® (ä¸‹æ¬¡å¯è·³è¿‡é¢„å¤„ç†)\n",
    "!cp -r data/hwdb2.0 /content/drive/MyDrive/handwritten-chinese-ocr/\n",
    "\n",
    "print(\"âœ… å·²ä¿å­˜åˆ° Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3f054",
   "metadata": {},
   "source": [
    "## 11. æµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09674a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "!python test.py -m hctr -f hctr_checkpoint.pth.tar -i data/hwdb2.0 -bm -dm greedy-search --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263da2b7",
   "metadata": {},
   "source": [
    "## 12. æ¢å¤è®­ç»ƒ (å¯é€‰)\n",
    "\n",
    "å¦‚æœ Colab æ–­å¼€ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¢å¤è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å¤åˆ¶ä¿å­˜çš„æ•°æ®å’Œ checkpoint\n",
    "# !cp -r /content/drive/MyDrive/handwritten-chinese-ocr/hwdb2.0 data/\n",
    "# !cp /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/hctr_checkpoint.pth.tar .\n",
    "\n",
    "# # ç»§ç»­è®­ç»ƒ\n",
    "# !python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 50 --gpu 0 -re hctr_checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f46063",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥ GPU çŠ¶æ€å’Œ Python ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a055259",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece3ca2",
   "metadata": {},
   "source": [
    "## 3. å¤åˆ¶é¡¹ç›®åˆ° Colab æœ¬åœ°å­˜å‚¨\n",
    "\n",
    "âš ï¸ **è¯·ä¿®æ”¹ä¸‹é¢è·¯å¾„ä¸ºä½  Google Drive ä¸­çš„å®é™…ä½ç½®**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbaf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= è¯·ä¿®æ”¹æ­¤è·¯å¾„ =======\n",
    "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/handwritten-chinese-ocr-samples\"\n",
    "# ============================\n",
    "\n",
    "# å¤åˆ¶åˆ° Colab æœ¬åœ°ç£ç›˜ï¼ˆè¯»å†™é€Ÿåº¦æ›´å¿«ï¼‰\n",
    "!cp -r {DRIVE_PROJECT_PATH} /content/\n",
    "%cd /content/handwritten-chinese-ocr-samples\n",
    "\n",
    "# æŸ¥çœ‹é¡¹ç›®æ–‡ä»¶\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38558dd6",
   "metadata": {},
   "source": [
    "## 4. å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75406d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q editdistance\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8e0c1",
   "metadata": {},
   "source": [
    "## 5. é¢„å¤„ç† DGRL æ•°æ®é›† (é¦–æ¬¡è¿è¡Œ)\n",
    "\n",
    "å°† CASIA-HWDB2.0 çš„ `.dgrl` æ–‡ä»¶è½¬æ¢ä¸º PNG å›¾ç‰‡ + æ ‡ç­¾æ–‡ä»¶ã€‚\n",
    "\n",
    "âš¡ **æ­¤æ­¥éª¤è€—æ—¶è¾ƒé•¿ (~10-20åˆ†é’Ÿ)ï¼Œä½†åªéœ€è¿è¡Œä¸€æ¬¡**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ç»é¢„å¤„ç†è¿‡\n",
    "if os.path.exists('data/hwdb2.0/train_img_id_gt.txt'):\n",
    "    print(\"âœ… æ•°æ®å·²é¢„å¤„ç†ï¼Œè·³è¿‡æ­¤æ­¥éª¤\")\n",
    "    !wc -l data/hwdb2.0/*_img_id_gt.txt\n",
    "else:\n",
    "    print(\"ğŸ”„ å¼€å§‹é¢„å¤„ç† DGRL æ–‡ä»¶ (è¿™éœ€è¦ 10-20 åˆ†é’Ÿ)...\")\n",
    "    \n",
    "    # å¯ç”¨åƒåœ¾å›æ”¶ä¼˜åŒ–å†…å­˜\n",
    "    gc.enable()\n",
    "    \n",
    "    !python preprocess_dgrl.py \\\n",
    "        --train_dir HWDB2.0Train \\\n",
    "        --test_dir HWDB2.0Test \\\n",
    "        --output_dir data/hwdb2.0\n",
    "    \n",
    "    print(\"\\nâœ… é¢„å¤„ç†å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e9267",
   "metadata": {},
   "source": [
    "## 6. æ£€æŸ¥æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡\n",
    "print(\"=== æ•°æ®é›†ç»Ÿè®¡ ===\")\n",
    "!wc -l data/hwdb2.0/*_img_id_gt.txt\n",
    "\n",
    "print(\"\\n=== æ ·æœ¬ç¤ºä¾‹ ===\")\n",
    "!head -3 data/hwdb2.0/train_img_id_gt.txt\n",
    "\n",
    "print(\"\\n=== å­—ç¬¦è¡¨å¤§å° ===\")\n",
    "!wc -l data/hwdb2.0/chars_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ ·æœ¬\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "with open('data/hwdb2.0/train_img_id_gt.txt', 'r', encoding='utf-8') as f:\n",
    "    samples = [line.strip().split(',', 1) for line in f.readlines()[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 5))\n",
    "for i, (img_name, text) in enumerate(samples):\n",
    "    img_path = os.path.join('data/hwdb2.0/train', img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(text[:40] + '...' if len(text) > 40 else text)\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace23903",
   "metadata": {},
   "source": [
    "## 7. å¼€å§‹è®­ç»ƒ ğŸš€\n",
    "\n",
    "**å‚æ•°è¯´æ˜:**\n",
    "- `-m hctr`: æ‰‹å†™ä¸­æ–‡æ–‡æœ¬è¯†åˆ«æ¨¡å‹\n",
    "- `-d`: æ•°æ®é›†è·¯å¾„  \n",
    "- `-b`: batch size (T4 GPU å»ºè®® 8-16)\n",
    "- `-ep`: è®­ç»ƒè½®æ•°\n",
    "- `-pf`: æ‰“å°é¢‘ç‡\n",
    "- `--gpu 0`: ä½¿ç”¨ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b70c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ¨¡å‹ (å®Œæ•´æ•°æ®é›†çº¦éœ€ 2-4 å°æ—¶/epoch)\n",
    "!python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 30 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06317be",
   "metadata": {},
   "source": [
    "## 8. ä¿å­˜æ£€æŸ¥ç‚¹åˆ° Google Drive âš ï¸\n",
    "\n",
    "**é‡è¦**: Colab ä¼šè¯å¯èƒ½éšæ—¶æ–­å¼€ï¼Œå®šæœŸä¿å­˜æ¨¡å‹åˆ° Drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0afc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "!mkdir -p /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints\n",
    "\n",
    "# ä¿å­˜æ‰€æœ‰ checkpoint\n",
    "!cp hctr_checkpoint.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/\n",
    "!cp hctr_*acc*.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/ 2>/dev/null || true\n",
    "\n",
    "# ä¿å­˜é¢„å¤„ç†å¥½çš„æ•°æ® (æ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥ä½¿ç”¨)\n",
    "!cp -r data/hwdb2.0 /content/drive/MyDrive/handwritten-chinese-ocr/\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å’Œæ•°æ®å·²ä¿å­˜åˆ° Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335122c",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc816cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "!python test.py -m hctr -f hctr_checkpoint.pth.tar -i data/hwdb2.0 -bm -dm greedy-search --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e482e3",
   "metadata": {},
   "source": [
    "## 10. ä»æ–­ç‚¹æ¢å¤è®­ç»ƒ (å¯é€‰)\n",
    "\n",
    "å¦‚æœ Colab æ–­å¼€ï¼Œå¯ä»¥ä» Drive æ¢å¤ç»§ç»­è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç æ¥æ¢å¤è®­ç»ƒ\n",
    "\n",
    "# å¦‚æœä¹‹å‰å·²é¢„å¤„ç†æ•°æ®ï¼Œå¯ç›´æ¥å¤åˆ¶\n",
    "# !cp -r /content/drive/MyDrive/handwritten-chinese-ocr/hwdb2.0 data/\n",
    "\n",
    "# å¤åˆ¶ checkpoint\n",
    "# !cp /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/hctr_checkpoint.pth.tar .\n",
    "\n",
    "# ç»§ç»­è®­ç»ƒ\n",
    "# !python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 50 --gpu 0 -re hctr_checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada01bf",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥ GPU çŠ¶æ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82564800",
   "metadata": {},
   "source": [
    "## 1. æ£€æŸ¥ GPU çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e171f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b535e",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816639c",
   "metadata": {},
   "source": [
    "## 3. å¤åˆ¶é¡¹ç›®åˆ° Colab è¿è¡Œç¯å¢ƒ\n",
    "\n",
    "**é‡è¦**: è¯·ä¿®æ”¹ä¸‹é¢çš„è·¯å¾„ä¸ºä½ å®é™…ä¸Šä¼ åˆ° Google Drive çš„ä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134aaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿®æ”¹ä¸ºä½ çš„ Google Drive ä¸­é¡¹ç›®æ–‡ä»¶å¤¹çš„è·¯å¾„\n",
    "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/handwritten-chinese-ocr-samples\"\n",
    "\n",
    "# å¤åˆ¶åˆ° Colab æœ¬åœ°ï¼ˆè¯»å†™æ›´å¿«ï¼‰\n",
    "!cp -r {DRIVE_PROJECT_PATH} /content/\n",
    "%cd /content/handwritten-chinese-ocr-samples\n",
    "\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b89ad",
   "metadata": {},
   "source": [
    "## 4. å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q editdistance\n",
    "print(\"ä¾èµ–å®‰è£…å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998afc0d",
   "metadata": {},
   "source": [
    "## 5. é¢„å¤„ç† DGRL æ•°æ®é›†\n",
    "\n",
    "å°† CASIA-HWDB2.0 çš„ `.dgrl` æ–‡ä»¶è½¬æ¢ä¸º PNG å›¾ç‰‡å’Œæ ‡ç­¾æ–‡ä»¶ã€‚\n",
    "\n",
    "**é¦–æ¬¡è¿è¡Œéœ€è¦æ‰§è¡Œæ­¤æ­¥éª¤**ï¼Œä¹‹åå¦‚æœ `data/hwdb2.0` å·²å­˜åœ¨å¯è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ç»é¢„å¤„ç†è¿‡\n",
    "if os.path.exists('data/hwdb2.0/train_img_id_gt.txt'):\n",
    "    print(\"æ•°æ®å·²é¢„å¤„ç†ï¼Œè·³è¿‡æ­¤æ­¥éª¤\")\n",
    "    !wc -l data/hwdb2.0/*_img_id_gt.txt\n",
    "else:\n",
    "    print(\"å¼€å§‹é¢„å¤„ç† DGRL æ–‡ä»¶...\")\n",
    "    !python preprocess_dgrl.py --train_dir HWDB2.0Train --test_dir HWDB2.0Test --output_dir data/hwdb2.0\n",
    "    print(\"\\né¢„å¤„ç†å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658a17e",
   "metadata": {},
   "source": [
    "## 6. æ£€æŸ¥æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡\n",
    "!echo \"=== æ•°æ®é›†ç»Ÿè®¡ ===\"\n",
    "!wc -l data/hwdb2.0/*_img_id_gt.txt\n",
    "\n",
    "!echo \"\\n=== æ ·æœ¬ç¤ºä¾‹ ===\"\n",
    "!head -3 data/hwdb2.0/train_img_id_gt.txt\n",
    "\n",
    "!echo \"\\n=== å­—ç¬¦æ•°é‡ ===\"\n",
    "!wc -l data/hwdb2.0/chars_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# è¯»å–å‰3ä¸ªæ ·æœ¬\n",
    "with open('data/hwdb2.0/train_img_id_gt.txt', 'r', encoding='utf-8') as f:\n",
    "    samples = [line.strip().split(',', 1) for line in f.readlines()[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 6))\n",
    "for i, (img_name, text) in enumerate(samples):\n",
    "    img_path = os.path.join('data/hwdb2.0/train', img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(text[:30] + '...' if len(text) > 30 else text)\n",
    "        axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451e5ee",
   "metadata": {},
   "source": [
    "## 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "### å‚æ•°è¯´æ˜:\n",
    "- `-m hctr`: æ¨¡å‹ç±»å‹\n",
    "- `-d`: æ•°æ®é›†è·¯å¾„\n",
    "- `-b`: batch size (T4 GPU å»ºè®® 8-16)\n",
    "- `-ep`: è®­ç»ƒè½®æ•°\n",
    "- `-pf`: æ‰“å°é¢‘ç‡\n",
    "- `--gpu 0`: ä½¿ç”¨ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£å¼è®­ç»ƒ (å®Œæ•´æ•°æ®é›†)\n",
    "!python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 30 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e35a5",
   "metadata": {},
   "source": [
    "## 8. æµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "!python test.py -m hctr -f hctr_checkpoint.pth.tar -i data/hwdb2.0 -bm -dm greedy-search --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•å¼ å›¾ç‰‡æ¨ç†\n",
    "!python test.py -m hctr -f hctr_checkpoint.pth.tar -i data/hwdb2.0/test/006-P16_000.png -dm greedy-search --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875c31b",
   "metadata": {},
   "source": [
    "## 9. ä¿å­˜æ¨¡å‹åˆ° Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fe0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ checkpoint åˆ° Driveï¼ˆé˜²æ­¢ Colab æ–­å¼€ä¸¢å¤±ï¼‰\n",
    "!mkdir -p /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints\n",
    "!cp hctr_checkpoint.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/\n",
    "!cp hctr_*acc*.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/ 2>/dev/null || true\n",
    "\n",
    "# åŒæ—¶ä¿å­˜é¢„å¤„ç†å¥½çš„æ•°æ®ï¼ˆä¸‹æ¬¡å¯ç›´æ¥ä½¿ç”¨ï¼‰\n",
    "!cp -r data/hwdb2.0 /content/drive/MyDrive/handwritten-chinese-ocr/\n",
    "\n",
    "print(\"æ¨¡å‹å’Œæ•°æ®å·²ä¿å­˜åˆ° Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5699bd",
   "metadata": {},
   "source": [
    "## 10. ä»æ–­ç‚¹æ¢å¤è®­ç»ƒï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœ Colab æ–­å¼€ï¼Œå¯ä»¥ä»ä¿å­˜çš„ checkpoint ç»§ç»­è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä» Drive å¤åˆ¶å› checkpoint\n",
    "# !cp /content/drive/MyDrive/handwritten-chinese-ocr/checkpoints/hctr_checkpoint.pth.tar .\n",
    "\n",
    "# æ¢å¤è®­ç»ƒ\n",
    "# !python main.py -m hctr -d data/hwdb2.0 -b 16 -pf 100 -ep 50 --gpu 0 -re hctr_checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4215db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ˜¯å¦åˆ†é…åˆ° GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01144e3",
   "metadata": {},
   "source": [
    "## 2. æŒ‚è½½ Google Drive\n",
    "\n",
    "å°†æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜åˆ° Google Driveï¼Œé˜²æ­¢ Colab æ–­å¼€åä¸¢å¤±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03286884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# åˆ›å»ºé¡¹ç›®å·¥ä½œç›®å½• (å¯é€‰)\n",
    "!mkdir -p /content/drive/MyDrive/handwritten-chinese-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e3fd6",
   "metadata": {},
   "source": [
    "## 3. è·å–é¡¹ç›®ä»£ç \n",
    "\n",
    "**æ–¹å¼ä¸€**: å¦‚æœä½ å·²å°†ä»£ç ä¸Šä¼ åˆ° GitHubï¼Œç›´æ¥å…‹éš†ï¼š\n",
    "```python\n",
    "!git clone https://github.com/YOUR_USERNAME/handwritten-chinese-ocr-samples.git\n",
    "%cd handwritten-chinese-ocr-samples\n",
    "```\n",
    "\n",
    "**æ–¹å¼äºŒ**: å¦‚æœä»£ç åœ¨ Google Drive ä¸­ï¼Œç›´æ¥å¤åˆ¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49563535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹å¼äºŒ: ä» Google Drive å¤åˆ¶ä»£ç  (å‡è®¾ä½ å·²å°†é¡¹ç›®æ–‡ä»¶å¤¹ä¸Šä¼ åˆ° Drive)\n",
    "# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹\n",
    "!cp -r /content/drive/MyDrive/handwritten-chinese-ocr-samples /content/\n",
    "%cd /content/handwritten-chinese-ocr-samples\n",
    "\n",
    "# æŸ¥çœ‹ç›®å½•ç»“æ„\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acb859",
   "metadata": {},
   "source": [
    "## 4. å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print(\"ä¾èµ–å®‰è£…å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced86370",
   "metadata": {},
   "source": [
    "## 5. å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "### é€‰é¡¹ A: ä½¿ç”¨ Demo æ•°æ® (å¿«é€Ÿæµ‹è¯•)\n",
    "å¦‚æœåªæ˜¯æƒ³éªŒè¯ä»£ç èƒ½è·‘é€šï¼Œè¿è¡Œä»¥ä¸‹ Cell ç”Ÿæˆ demo æ•°æ®ã€‚\n",
    "\n",
    "### é€‰é¡¹ B: ä½¿ç”¨å®Œæ•´æ•°æ®é›† (æ­£å¼è®­ç»ƒ)\n",
    "1. ä¸‹è½½ CASIA-HWDB æˆ– SCUT-EPT æ•°æ®é›†\n",
    "2. ä¸Šä¼ åˆ° Google Drive\n",
    "3. ä¿®æ”¹ä¸‹æ–¹è·¯å¾„æŒ‡å‘ä½ çš„æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1addbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰é¡¹ A: ç”Ÿæˆ Demo æ•°æ® (ç”¨äºå¿«é€ŸéªŒè¯)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# åˆ›å»ºç›®å½•\n",
    "os.makedirs('data/demo_data/train', exist_ok=True)\n",
    "os.makedirs('data/demo_data/val', exist_ok=True)\n",
    "os.makedirs('data/demo_data/test', exist_ok=True)\n",
    "\n",
    "# è¯»å–æ ·ä¾‹å›¾ç‰‡çš„æ ‡ç­¾\n",
    "gt_file = 'images/scut_test_img_id_gt.txt'\n",
    "samples = []\n",
    "with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(',', 1)\n",
    "            if len(parts) == 2:\n",
    "                samples.append((parts[0] + '.jpg', parts[1]))\n",
    "\n",
    "# å¤åˆ¶å›¾ç‰‡å¹¶ç”Ÿæˆæ ‡ç­¾æ–‡ä»¶\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    gt_lines = []\n",
    "    for img_name, label in samples:\n",
    "        src = os.path.join('images', img_name)\n",
    "        dst = os.path.join('data/demo_data', phase, img_name)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            gt_lines.append(f\"{img_name},{label}\")\n",
    "    \n",
    "    with open(f'data/demo_data/{phase}_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(gt_lines))\n",
    "\n",
    "# å¤åˆ¶å­—ç¬¦åˆ—è¡¨\n",
    "shutil.copy('data/handwritten_ctr_data/chars_list.txt', 'data/demo_data/chars_list.txt')\n",
    "\n",
    "print(\"Demo æ•°æ®å‡†å¤‡å®Œæˆ!\")\n",
    "!ls -la data/demo_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232309a",
   "metadata": {},
   "source": [
    "## 6. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "### å‚æ•°è¯´æ˜:\n",
    "- `-m hctr`: æ¨¡å‹ç±»å‹ (handwritten Chinese text recognition)\n",
    "- `-d`: æ•°æ®é›†è·¯å¾„\n",
    "- `-b`: batch size (æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ï¼ŒT4 å»ºè®® 8-16)\n",
    "- `-ep`: è®­ç»ƒè½®æ•°\n",
    "- `-pf`: æ‰“å°é¢‘ç‡\n",
    "- `--gpu 0`: ä½¿ç”¨ç¬¬ä¸€å— GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo è®­ç»ƒ (å¿«é€ŸéªŒè¯ï¼Œçº¦ 1-2 åˆ†é’Ÿ)\n",
    "!python main.py -m hctr -d data/demo_data -b 2 -pf 1 -ep 5 --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£å¼è®­ç»ƒ (éœ€è¦å®Œæ•´æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶)\n",
    "# è¯·å°† DATA_PATH æ›¿æ¢ä¸ºä½ çš„å®é™…æ•°æ®é›†è·¯å¾„\n",
    "\n",
    "# DATA_PATH = \"/content/drive/MyDrive/your_dataset_folder\"\n",
    "# !python main.py -m hctr -d {DATA_PATH} -b 16 -pf 100 -ep 50 --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b58626",
   "metadata": {},
   "source": [
    "## 7. æµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "!python test.py -m hctr -f hctr_checkpoint.pth.tar -i images/000000.jpg -dm greedy-search --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbc25a",
   "metadata": {},
   "source": [
    "## 8. ä¿å­˜æ¨¡å‹åˆ° Google Drive\n",
    "\n",
    "å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¤åˆ¶åˆ° Driveï¼Œé˜²æ­¢ Colab æ–­å¼€åä¸¢å¤±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e462e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æœ€æ–°çš„ checkpoint åˆ° Google Drive\n",
    "!cp hctr_checkpoint.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/\n",
    "!cp hctr_*.pth.tar /content/drive/MyDrive/handwritten-chinese-ocr/ 2>/dev/null || true\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜åˆ° Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
