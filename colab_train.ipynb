{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handwritten Chinese OCR Training Notebook\n",
        "\n",
        "This notebook trains a ResNet + CTC based OCR model for handwritten Chinese text recognition using the CASIA-HWDB2.x dataset.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Drive with the project folder containing `main.py`, `test.py`, and preprocessed dataset\n",
        "- GPU runtime recommended (Runtime ‚Üí Change runtime type ‚Üí GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"‚úÖ Google Drive already mounted\")\n",
        "else:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set Project Path\n",
        "\n",
        "**‚ö†Ô∏è Modify `PROJECT_PATH` below to match your Google Drive folder location.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è MODIFY THIS PATH to your project folder in Google Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/handwritten-chinese-ocr-samples'\n",
        "\n",
        "# Change to project directory\n",
        "%cd {PROJECT_PATH}\n",
        "\n",
        "# Verify project structure\n",
        "!ls -la\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies from requirements.txt\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Verify PyTorch installation\n",
        "import torch\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preparation Notes\n",
        "\n",
        "### ‚ö†Ô∏è IMPORTANT: Dataset Preprocessing Required\n",
        "\n",
        "Before training, the raw **CASIA-HWDB2.x** binary files (`.dgrl`, `.dgr`, `.gnt` formats) must be preprocessed into image files (PNG) and label files.\n",
        "\n",
        "### Preprocessing Utilities\n",
        "\n",
        "The following utility scripts are available in `utils/casia-hwdb-data-preparation/`:\n",
        "\n",
        "| Script | Purpose | Input Format |\n",
        "|--------|---------|-------------|\n",
        "| `preprocess_dgrl.py` | Convert DGRL text line files to PNG + labels | `.dgrl` |\n",
        "| `dgr2png.c` | C/C++ utility for converting DGR files | `.dgr` |\n",
        "| `gnt2png.py` | Convert GNT character files to PNG | `.gnt` |\n",
        "\n",
        "### Expected Dataset Structure After Preprocessing\n",
        "\n",
        "```\n",
        "data/hwdb2.0/\n",
        "‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ 000000.png\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ val/\n",
        "‚îú‚îÄ‚îÄ test/\n",
        "‚îú‚îÄ‚îÄ train_img_id_gt.txt    # Format: image_name,label_text\n",
        "‚îú‚îÄ‚îÄ val_img_id_gt.txt\n",
        "‚îú‚îÄ‚îÄ test_img_id_gt.txt\n",
        "‚îî‚îÄ‚îÄ chars_list.txt         # One character per line\n",
        "```\n",
        "\n",
        "### Example: Preprocessing DGRL Files\n",
        "\n",
        "```bash\n",
        "python preprocess_dgrl.py --train_dir HWDB2.0Train --test_dir HWDB2.0Test --output_dir data/hwdb2.0 --val_split 0.1\n",
        "```\n",
        "\n",
        "### Compiling dgr2png.c (if needed)\n",
        "\n",
        "```bash\n",
        "cd utils/casia-hwdb-data-preparation\n",
        "gcc -o dgr2png dgr2png.c\n",
        "./dgr2png <input.dgr> <output_dir>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset path (modify if using a different location)\n",
        "DATASET_PATH = 'data/hwdb2.0'\n",
        "\n",
        "import os\n",
        "\n",
        "required_files = [\n",
        "    f'{DATASET_PATH}/train_img_id_gt.txt',\n",
        "    f'{DATASET_PATH}/val_img_id_gt.txt', \n",
        "    f'{DATASET_PATH}/test_img_id_gt.txt',\n",
        "    f'{DATASET_PATH}/chars_list.txt',\n",
        "    f'{DATASET_PATH}/train',\n",
        "    f'{DATASET_PATH}/val',\n",
        "    f'{DATASET_PATH}/test'\n",
        "]\n",
        "\n",
        "print(\"Dataset verification:\")\n",
        "print(\"=\"*50)\n",
        "all_ok = True\n",
        "for f in required_files:\n",
        "    exists = os.path.exists(f)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"{status} {f}\")\n",
        "    if not exists:\n",
        "        all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        gt_file = f'{DATASET_PATH}/{split}_img_id_gt.txt'\n",
        "        with open(gt_file, 'r', encoding='utf-8') as f:\n",
        "            count = len(f.readlines())\n",
        "        print(f\"   {split}: {count} samples\")\n",
        "    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n",
        "        num_chars = len(f.readlines())\n",
        "    print(f\"   Character vocabulary: {num_chars}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Some files are missing. Please preprocess the dataset first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "DATASET_PATH = 'data/hwdb2.0'\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "PRINT_FREQ = 50\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Run training\n",
        "!python main.py -m hctr \\\n",
        "    -d {DATASET_PATH} \\\n",
        "    -b {BATCH_SIZE} \\\n",
        "    -ep {EPOCHS} \\\n",
        "    -pf {PRINT_FREQ} \\\n",
        "    -j {NUM_WORKERS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Find Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "model_files = glob.glob('hctr_*.pth.tar')\n",
        "\n",
        "if model_files:\n",
        "    print(\"Saved models:\")\n",
        "    for f in sorted(model_files):\n",
        "        size_mb = os.path.getsize(f) / (1024*1024)\n",
        "        print(f\"  üìÅ {f} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    acc_models = [f for f in model_files if 'acc' in f]\n",
        "    if acc_models:\n",
        "        BEST_MODEL = sorted(acc_models)[-1]\n",
        "    else:\n",
        "        BEST_MODEL = 'hctr_checkpoint.pth.tar'\n",
        "    print(f\"\\n‚úÖ Selected model: {BEST_MODEL}\")\n",
        "else:\n",
        "    print(\"‚ùå No model files found. Please run training first.\")\n",
        "    BEST_MODEL = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation configuration\n",
        "DATASET_PATH = 'data/hwdb2.0'\n",
        "MODEL_FILE = BEST_MODEL\n",
        "TEST_PATH = f'{DATASET_PATH}/test'\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "if MODEL_FILE and os.path.exists(MODEL_FILE):\n",
        "    print(f\"Evaluating model: {MODEL_FILE}\")\n",
        "    print(f\"Test set: {TEST_PATH}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    !python test.py -m hctr \\\n",
        "        -f {MODEL_FILE} \\\n",
        "        -i {TEST_PATH} \\\n",
        "        -b {BATCH_SIZE} \\\n",
        "        -bm \\\n",
        "        -dm greedy-search \\\n",
        "        -pf 20\n",
        "else:\n",
        "    print(\"‚ùå Model file not found. Please run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "SAVE_DIR = f'{PROJECT_PATH}/checkpoints'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "model_files = glob.glob('hctr_*.pth.tar')\n",
        "for f in model_files:\n",
        "    dst = os.path.join(SAVE_DIR, f)\n",
        "    shutil.copy2(f, dst)\n",
        "    print(f\"‚úÖ Saved: {dst}\")\n",
        "\n",
        "print(f\"\\nÔøΩÔøΩ All models saved to: {SAVE_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
