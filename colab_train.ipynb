{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Chinese OCR - Colab Training\n\nTrain handwritten Chinese OCR model on Google Colab using CASIA-HWDB2.0 dataset.\n\n## Quick Start\n1. Upload project folder (with `HWDB2.0Train` and `HWDB2.0Test`) to Google Drive\n2. Open this notebook in Colab with GPU runtime\n3. Run cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\nimport os\n\nif os.path.exists('/content/drive/MyDrive'):\n    print('Google Drive already mounted')\nelse:\n    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Project Path\n\nModify `PROJECT_PATH` to match your Google Drive folder location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this path to your project folder in Google Drive\nPROJECT_PATH = '/content/drive/MyDrive/handwritten-chinese-ocr-samples'\n\n%cd {PROJECT_PATH}\n!ls -la\nprint(f'\\nWorking directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n\nimport torch\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Dataset\n\nThe CASIA-HWDB `.dgrl` files must be converted to PNG images and text labels.\n\n**Two preprocessing options:**\n- `preprocess_dgrl.py` - Extracts text **line images** (for line-level OCR)\n- `preprocess_dgrl_pages.py` - Extracts **full page images** (for page-level OCR)\n\n**Expected output structure:**\n```\ndata/hwdb2.0/\n├── train/          # PNG images\n├── val/\n├── test/\n├── img_gt.txt      # Format: image_name,label_text\n└── chars_list.txt  # Character vocabulary\n```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Extract text line images (recommended for training)\n!python preprocess_dgrl.py \\\n    --input_dir HWDB2.0Train \\\n    --output_dir data/hwdb2.0 \\\n    --target_height 128 \\\n    --workers 4\n\n# Option B: Extract full page images (for page-level analysis)\n# Uncomment to use this instead:\n# !python preprocess_dgrl_pages.py \\\n#     --input_dir HWDB2.0Train \\\n#     --output_dir data/pages \\\n#     --no-viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/hwdb2.0'\n\nrequired_files = [\n    f'{DATASET_PATH}/img_gt.txt',\n    f'{DATASET_PATH}/chars_list.txt'\n]\n\nprint('Dataset verification:')\nfor f in required_files:\n    exists = os.path.exists(f)\n    print(f\"{'✓' if exists else '✗'} {f}\")\n\nif os.path.exists(f'{DATASET_PATH}/img_gt.txt'):\n    with open(f'{DATASET_PATH}/img_gt.txt', 'r', encoding='utf-8') as f:\n        count = len(f.readlines())\n    print(f'\\nTotal samples: {count}')\n    \n    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n        num_chars = len(f.readlines())\n    print(f'Character vocabulary: {num_chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/hwdb2.0'\nBATCH_SIZE = 8\nEPOCHS = 30\nPRINT_FREQ = 50\nNUM_WORKERS = 2\n\n!python main.py -m hctr \\\n    -d {DATASET_PATH} \\\n    -b {BATCH_SIZE} \\\n    -ep {EPOCHS} \\\n    -pf {PRINT_FREQ} \\\n    -j {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n\nmodel_files = glob.glob('hctr_*.pth.tar')\n\nif model_files:\n    print('Saved models:')\n    for f in sorted(model_files):\n        size_mb = os.path.getsize(f) / (1024*1024)\n        print(f'  {f} ({size_mb:.1f} MB)')\n    \n    # Select best model\n    acc_models = [f for f in model_files if 'acc' in f]\n    BEST_MODEL = sorted(acc_models)[-1] if acc_models else 'hctr_checkpoint.pth.tar'\n    print(f'\\nSelected: {BEST_MODEL}')\nelse:\n    print('No models found')\n    BEST_MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/hwdb2.0'\nMODEL_FILE = BEST_MODEL\nBATCH_SIZE = 16\n\nif MODEL_FILE and os.path.exists(MODEL_FILE):\n    print(f'Evaluating: {MODEL_FILE}')\n    !python test.py -m hctr \\\n        -f {MODEL_FILE} \\\n        -i {DATASET_PATH} \\\n        -b {BATCH_SIZE} \\\n        -bm \\\n        -dm greedy-search \\\n        -pf 20\nelse:\n    print('Model not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n\nSAVE_DIR = f'{PROJECT_PATH}/checkpoints'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nmodel_files = glob.glob('hctr_*.pth.tar')\nfor f in model_files:\n    dst = os.path.join(SAVE_DIR, f)\n    shutil.copy2(f, dst)\n    print(f'Saved: {dst}')\n\nprint(f'\\nAll models saved to: {SAVE_DIR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}