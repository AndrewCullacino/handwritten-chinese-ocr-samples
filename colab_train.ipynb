{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EFWD6anw00v"
      },
      "source": [
        "# Handwritten Chinese OCR - Colab Training (A100 Optimized)\n",
        "\n",
        "Train handwritten Chinese OCR model on Google Colab using CASIA-HWDB2.0 dataset.\n",
        "\n",
        "**Optimized for A100 GPU:**\n",
        "- TF32 precision (8-19x faster than FP32)\n",
        "- Mixed precision training (AMP)\n",
        "- Optimized data loading with prefetching\n",
        "- Large batch sizes for better GPU utilization\n",
        "\n",
        "## Quick Start\n",
        "1. Upload **only data** (`HWDB2.0Train`, `HWDB2.0Test`) to `My Drive/HWDB-data/`\n",
        "2. Code is automatically cloned from GitHub (always up-to-date)\n",
        "3. Open this notebook in Colab with **A100 GPU runtime**\n",
        "4. Run cells in order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1yb5MRRw00x"
      },
      "source": [
        "## 1. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "orxO49HLw00y",
        "outputId": "e53d01e6-0f11-49a6-f29f-149b9eafd746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 19 05:45:08 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory: 42.5 GB\n",
            "\n",
            "✓ A100 detected! Optimal settings will be used:\n",
            "  - Batch size: 32-64\n",
            "  - TF32 precision: enabled\n",
            "  - Mixed precision (AMP): enabled\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'CUDA: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'GPU: {gpu_name}')\n",
        "    print(f'Memory: {gpu_memory:.1f} GB')\n",
        "\n",
        "    if 'A100' in gpu_name:\n",
        "        print('\\n✓ A100 detected! Optimal settings will be used:')\n",
        "        print('  - Batch size: 32-64')\n",
        "        print('  - TF32 precision: enabled')\n",
        "        print('  - Mixed precision (AMP): enabled')\n",
        "    elif 'V100' in gpu_name:\n",
        "        print('\\n✓ V100 detected. Recommended batch size: 16-32')\n",
        "    elif 'T4' in gpu_name:\n",
        "        print('\\n✓ T4 detected. Recommended batch size: 8-16')\n",
        "    else:\n",
        "        print(f'\\n✓ {gpu_name} detected.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZeogIFGw00z"
      },
      "source": [
        "## 2. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NdjbvWwNw00z",
        "outputId": "f3da74a9-c7ee-4678-d612-3d2c0b9e3bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Drive mounted\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print('✓ Drive mounted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYhWSMTfw00z"
      },
      "source": [
        "## 3. Clone Project from GitHub\n",
        "\n",
        "Code is cloned fresh from GitHub, so you always get the latest updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xodR7m-Bw00z",
        "outputId": "4b8a9b67-297f-4bac-9732-fd07b91a7f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'handwritten-chinese-ocr-samples'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 231 (delta 70), reused 78 (delta 40), pack-reused 108 (from 1)\u001b[K\n",
            "Receiving objects: 100% (231/231), 746.76 KiB | 26.67 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "/content/handwritten-chinese-ocr-samples\n",
            "✓ Latest code cloned from GitHub\n"
          ]
        }
      ],
      "source": [
        "# Remove old clone if exists\n",
        "!rm -rf handwritten-chinese-ocr-samples\n",
        "\n",
        "# Clone latest code from GitHub\n",
        "!git clone https://github.com/AndrewCullacino/handwritten-chinese-ocr-samples.git\n",
        "%cd handwritten-chinese-ocr-samples\n",
        "\n",
        "print('✓ Latest code cloned from GitHub')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqcARIxw000"
      },
      "source": [
        "## 4. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZrSRY8Cw000"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements.txt\n",
        "print('✓ Dependencies installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qkisccvw000"
      },
      "source": [
        "## 5. Link Data from Google Drive\n",
        "\n",
        "**Important:** Your data should be in `My Drive/HWDB-data/` with this structure:\n",
        "```\n",
        "My Drive/HWDB-data/\n",
        "├── HWDB2.0Train/\n",
        "│   ├── *.dgrl files\n",
        "└── HWDB2.0Test/\n",
        "    ├── *.dgrl files\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeR6oHdFw000"
      },
      "outputs": [],
      "source": [
        "# Verify data exists in Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/HWDB-data'\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f'✗ Error: {DATA_DIR} not found')\n",
        "    print('Please upload HWDB2.0Train and HWDB2.0Test to My Drive/HWDB-data/')\n",
        "else:\n",
        "    # Check for .dgrl files\n",
        "    train_dir = f'{DATA_DIR}/HWDB2.0Train'\n",
        "    test_dir = f'{DATA_DIR}/HWDB2.0Test'\n",
        "\n",
        "    train_exists = os.path.exists(train_dir)\n",
        "    test_exists = os.path.exists(test_dir)\n",
        "\n",
        "    print(f'Data verification:')\n",
        "    print(f\"  {'✓' if train_exists else '✗'} {train_dir}\")\n",
        "    print(f\"  {'✓' if test_exists else '✗'} {test_dir}\")\n",
        "\n",
        "    if train_exists:\n",
        "        dgrl_count = len([f for f in os.listdir(train_dir) if f.endswith('.dgrl')])\n",
        "        print(f'  → Found {dgrl_count} .dgrl files in training data')\n",
        "\n",
        "    if train_exists and test_exists:\n",
        "        print('\\n✓ Data ready for preprocessing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix78mvZow000"
      },
      "source": [
        "## 6. Preprocess Dataset\n",
        "\n",
        "Extract text line images from DGRL files using `dgrl2png.py`.\n",
        "\n",
        "Processed data will be saved to Colab local storage (faster I/O than Drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCowg0SSw000"
      },
      "outputs": [],
      "source": [
        "# Check if already preprocessed\n",
        "if os.path.exists('data/hwdb2.0/train_img_id_gt.txt'):\n",
        "    print('✓ Data already preprocessed, skipping...')\n",
        "    with open('data/hwdb2.0/train_img_id_gt.txt', 'r') as f:\n",
        "        train_samples = len(f.readlines())\n",
        "    with open('data/hwdb2.0/val_img_id_gt.txt', 'r') as f:\n",
        "        val_samples = len(f.readlines())\n",
        "    with open('data/hwdb2.0/test_img_id_gt.txt', 'r') as f:\n",
        "        test_samples = len(f.readlines())\n",
        "    print(f'  Train: {train_samples} | Val: {val_samples} | Test: {test_samples}')\n",
        "else:\n",
        "    print('Preprocessing DGRL files using dgrl2png.py...')\n",
        "\n",
        "    # Create output directories\n",
        "    !mkdir -p data/hwdb2.0/train data/hwdb2.0/val data/hwdb2.0/test\n",
        "\n",
        "    # Extract training data\n",
        "    print('\\n[1/2] Extracting training data...')\n",
        "    !python utils/casia-hwdb-data-preparation/dgrl2png.py \\\n",
        "        /content/drive/MyDrive/HWDB-data/HWDB2.0Train \\\n",
        "        data/hwdb2.0/train \\\n",
        "        --image_height 128\n",
        "\n",
        "    # Extract test data\n",
        "    print('\\n[2/2] Extracting test data...')\n",
        "    !python utils/casia-hwdb-data-preparation/dgrl2png.py \\\n",
        "        /content/drive/MyDrive/HWDB-data/HWDB2.0Test \\\n",
        "        data/hwdb2.0/test \\\n",
        "        --image_height 128\n",
        "\n",
        "    # Create train/val split and generate metadata files\n",
        "    print('\\n[3/3] Creating train/val split...')\n",
        "\n",
        "    import random\n",
        "    import shutil\n",
        "\n",
        "    # Read training ground truth\n",
        "    with open('data/hwdb2.0/train/dgrl_img_gt.txt', 'r', encoding='utf-8') as f:\n",
        "        train_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.seed(42)\n",
        "    random.shuffle(train_lines)\n",
        "\n",
        "    val_size = int(len(train_lines) * 0.1)\n",
        "    val_lines = train_lines[:val_size]\n",
        "    train_lines = train_lines[val_size:]\n",
        "\n",
        "    # Move val images to val folder\n",
        "    for line in val_lines:\n",
        "        img_name = line.split(',')[0]\n",
        "        src = f'data/hwdb2.0/train/{img_name}'\n",
        "        dst = f'data/hwdb2.0/val/{img_name}'\n",
        "        if os.path.exists(src):\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "    # Write metadata files\n",
        "    with open('data/hwdb2.0/train_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(train_lines))\n",
        "\n",
        "    with open('data/hwdb2.0/val_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(val_lines))\n",
        "\n",
        "    # Copy test ground truth\n",
        "    with open('data/hwdb2.0/test/dgrl_img_gt.txt', 'r', encoding='utf-8') as f:\n",
        "        test_lines = f.read()\n",
        "    with open('data/hwdb2.0/test_img_id_gt.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(test_lines)\n",
        "\n",
        "    # Generate character list from all data\n",
        "    all_chars = set()\n",
        "    for line in train_lines + val_lines + test_lines.strip().split('\\n'):\n",
        "        if ',' in line:\n",
        "            text = line.split(',', 1)[1]\n",
        "            all_chars.update(text)\n",
        "\n",
        "    with open('data/hwdb2.0/chars_list.txt', 'w', encoding='utf-8') as f:\n",
        "        for char in sorted(all_chars):\n",
        "            f.write(char + '\\n')\n",
        "\n",
        "    print(f'\\n✓ Preprocessing complete')\n",
        "    print(f'  Train: {len(train_lines)} | Val: {len(val_lines)} | Test: {len(test_lines.strip().split(chr(10)))}')\n",
        "    print(f'  Characters: {len(all_chars)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujK8ucQcw001"
      },
      "source": [
        "## 7. Verify Preprocessed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yBJpH7vw001"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/hwdb2.0'\n",
        "\n",
        "if os.path.exists(f'{DATASET_PATH}/train_img_id_gt.txt'):\n",
        "    # Count samples per split\n",
        "    splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        gt_file = f'{DATASET_PATH}/{split}_img_id_gt.txt'\n",
        "        if os.path.exists(gt_file):\n",
        "            with open(gt_file, 'r', encoding='utf-8') as f:\n",
        "                splits[split] = len(f.readlines())\n",
        "\n",
        "    # Count characters\n",
        "    with open(f'{DATASET_PATH}/chars_list.txt', 'r', encoding='utf-8') as f:\n",
        "        chars = len(f.readlines())\n",
        "\n",
        "    print(f'Dataset ready:')\n",
        "    print(f'  Train: {splits.get(\"train\", 0):,} samples')\n",
        "    print(f'  Val:   {splits.get(\"val\", 0):,} samples')\n",
        "    print(f'  Test:  {splits.get(\"test\", 0):,} samples')\n",
        "    print(f'  Character vocab: {chars:,}')\n",
        "    print(f'  Location: {DATASET_PATH}/')\n",
        "else:\n",
        "    print('✗ Dataset not found. Please run preprocessing first.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcpT6gSmw001"
      },
      "source": [
        "## 8. Training\n",
        "\n",
        "Train the model with A100-optimized settings:\n",
        "- **TF32 precision**: 8-19x faster than FP32 on A100\n",
        "- **Mixed precision (AMP)**: Further speedup with FP16/BF16\n",
        "- **Large batch sizes**: Better GPU utilization\n",
        "- **Gradient clipping**: Prevents exploding gradients in RNN\n",
        "\n",
        "Checkpoints will be saved locally in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jP92rfi0w001",
        "outputId": "54c98ea5-dfdf-4f5a-8cc5-dbca82a04da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ A100 detected - using batch_size=16\n",
            "\n",
            "Training configuration:\n",
            "  Batch size: 16\n",
            "  Epochs: 50\n",
            "  Workers: 4\n",
            "  Dataset: data/hwdb2.0\n",
            "  Max image width: 1200 (enforced in main.py)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "\n",
            "============================================================\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory: 42.5 GB\n",
            "✓ A100 detected - TF32 and optimizations enabled\n",
            "  - TF32 matmul: ON (8-19x faster than FP32)\n",
            "  - cuDNN benchmark: ON\n",
            "  - Mixed precision (AMP): ON\n",
            "============================================================\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/handwritten-chinese-ocr-samples/main.py\", line 617, in <module>\n",
            "    main()\n",
            "  File \"/content/handwritten-chinese-ocr-samples/main.py\", line 177, in main\n",
            "    main_worker(args.gpu, ngpus_per_node, args)\n",
            "  File \"/content/handwritten-chinese-ocr-samples/main.py\", line 192, in main_worker\n",
            "    model, characters = get_model_info(args)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/handwritten-chinese-ocr-samples/main.py\", line 597, in get_model_info\n",
            "    with open(chars_list_file, 'r', encoding='utf-8') as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/hwdb2.0/chars_list.txt'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "DATASET_PATH = 'data/hwdb2.0'\n",
        "\n",
        "# Auto-detect optimal batch size based on GPU\n",
        "# Note: max_width=1200 is enforced in main.py to prevent OOM on long text lines\n",
        "# The model is large (~38M params), so conservative batch sizes are needed\n",
        "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else ''\n",
        "if 'A100' in gpu_name:\n",
        "    BATCH_SIZE = 8        # A100 40GB: 16-24 (conservative for large model)\n",
        "    NUM_WORKERS = 4\n",
        "    print(f'✓ A100 detected - using batch_size={BATCH_SIZE}')\n",
        "elif 'V100' in gpu_name:\n",
        "    BATCH_SIZE = 6        # V100 16GB: 8-12\n",
        "    NUM_WORKERS = 4\n",
        "    print(f'✓ V100 detected - using batch_size={BATCH_SIZE}')\n",
        "elif 'T4' in gpu_name:\n",
        "    BATCH_SIZE = 8         # T4 16GB: 4-8\n",
        "    NUM_WORKERS = 2\n",
        "    print(f'✓ T4 detected - using batch_size={BATCH_SIZE}')\n",
        "else:\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_WORKERS = 2\n",
        "    print(f'Using default batch_size={BATCH_SIZE}')\n",
        "\n",
        "EPOCHS = 30             # Recommended: 30-50 epochs\n",
        "PRINT_FREQ = 100        # Print every N batches\n",
        "VAL_FREQ = 5000         # Validate every N batches\n",
        "\n",
        "print(f'\\nTraining configuration:')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Epochs: {EPOCHS}')\n",
        "print(f'  Workers: {NUM_WORKERS}')\n",
        "print(f'  Dataset: {DATASET_PATH}')\n",
        "print(f'  Max image width: 1200 (enforced in main.py)')\n",
        "\n",
        "!python main.py -m hctr \\\n",
        "    -d {DATASET_PATH} \\\n",
        "    -b {BATCH_SIZE} \\\n",
        "    -ep {EPOCHS} \\\n",
        "    -pf {PRINT_FREQ} \\\n",
        "    -vf {VAL_FREQ} \\\n",
        "    -j {NUM_WORKERS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zPhrlljw002"
      },
      "source": [
        "## 9. Find Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ixYeR0iw002"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "model_files = glob.glob('hctr_*.pth.tar')\n",
        "\n",
        "if model_files:\n",
        "    print('Saved models:')\n",
        "    for f in sorted(model_files):\n",
        "        size_mb = os.path.getsize(f) / (1024*1024)\n",
        "        print(f'  {f} ({size_mb:.1f} MB)')\n",
        "\n",
        "    acc_models = [f for f in model_files if 'acc' in f]\n",
        "    BEST_MODEL = sorted(acc_models)[-1] if acc_models else 'hctr_checkpoint.pth.tar'\n",
        "    print(f'\\n✓ Best model: {BEST_MODEL}')\n",
        "else:\n",
        "    print('No models found')\n",
        "    BEST_MODEL = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaPIn70aw002"
      },
      "source": [
        "## 10. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKhMCXPIw002"
      },
      "outputs": [],
      "source": [
        "if BEST_MODEL and os.path.exists(BEST_MODEL):\n",
        "    print(f'Evaluating: {BEST_MODEL}\\n')\n",
        "    !python test.py -m hctr \\\n",
        "        -f {BEST_MODEL} \\\n",
        "        -i {DATASET_PATH} \\\n",
        "        -b 16 \\\n",
        "        -bm \\\n",
        "        -dm greedy-search \\\n",
        "        -pf 20\n",
        "else:\n",
        "    print('Model not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofkt9iQIw002"
      },
      "source": [
        "## 11. Save Checkpoints to Drive\n",
        "\n",
        "**Important:** Save models to Drive to prevent loss when Colab disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "murDyESTw002"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Save to Drive\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/HWDB-data/checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "model_files = glob.glob('hctr_*.pth.tar')\n",
        "if model_files:\n",
        "    for f in model_files:\n",
        "        dst = os.path.join(CHECKPOINT_DIR, f)\n",
        "        shutil.copy2(f, dst)\n",
        "        print(f'✓ Saved: {dst}')\n",
        "    print(f'\\n✓ All models saved to Drive: {CHECKPOINT_DIR}')\n",
        "else:\n",
        "    print('No models to save')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS7xSOIZw002"
      },
      "source": [
        "## 12. Resume Training (Optional)\n",
        "\n",
        "If Colab disconnects, run cells 1-7 to restore environment, then run this cell to resume training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDp4vwNyw002"
      },
      "outputs": [],
      "source": [
        "# Copy checkpoint from Drive\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/HWDB-data/checkpoints'\n",
        "checkpoint_file = 'hctr_checkpoint.pth.tar'\n",
        "\n",
        "if os.path.exists(f'{CHECKPOINT_DIR}/{checkpoint_file}'):\n",
        "    !cp {CHECKPOINT_DIR}/{checkpoint_file} .\n",
        "    print(f'✓ Restored checkpoint: {checkpoint_file}')\n",
        "\n",
        "    # Detect GPU and set batch size (conservative values for large model)\n",
        "    import torch\n",
        "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else ''\n",
        "    BATCH_SIZE = 8 if 'A100' in gpu_name else (12 if 'V100' in gpu_name else 8)\n",
        "\n",
        "    # Resume training\n",
        "    !python main.py -m hctr \\\n",
        "        -d data/hwdb2.0 \\\n",
        "        -b {BATCH_SIZE} \\\n",
        "        -ep 100 \\\n",
        "        -pf 100 \\\n",
        "        -vf 5000 \\\n",
        "        -j 4 \\\n",
        "        -re {checkpoint_file}\n",
        "else:\n",
        "    print(f'✗ Checkpoint not found in {CHECKPOINT_DIR}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}